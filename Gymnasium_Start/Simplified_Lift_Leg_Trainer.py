import os
import numpy as np
from sb3_contrib import RecurrentPPO
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize, SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
import torch
import glob
import re
from datetime import datetime
import json
import csv
from stable_baselines3.common.callbacks import BaseCallback

# Import your environments
from Gymnasium_Start.Simple_Lift_Leg_BipedEnv import Simple_Lift_Leg_BipedEnv  # Nuevo entorno mejorado
from Archivos_Apoyo.Configuraciones_adicionales import cargar_posible_normalizacion
from Archivos_Apoyo.simple_log_redirect import log_print, both_print
from Archivos_Apoyo.CSVLogger import CSVLogger


class SimpleCsvKpiCallback(BaseCallback):
    """
    Lee info['kpi'] por step e info['ep_kpi'] al finalizar episodios
    y los vuelca a CSV en self.logs_dir.
    """
    def __init__(self, logs_dir: str, verbose: int = 0):
        super().__init__(verbose)
        self.logs_dir = logs_dir
        self._step_f = None
        self._ep_f = None
        self._step_writer = None
        self._ep_writer = None

    def _on_training_start(self) -> None:
        os.makedirs(self.logs_dir, exist_ok=True)
        self._step_f = open(os.path.join(self.logs_dir, "kpi_step.csv"), "w", newline="")
        self._ep_f   = open(os.path.join(self.logs_dir, "kpi_episode.csv"), "w", newline="")
        self._step_writer = csv.DictWriter(self._step_f,
            fieldnames=["timesteps","env_idx","reward","roll","pitch",
                        "left_down","right_down","F_L","F_R",
                        "zmp_x","zmp_y","com_x","com_y","com_z"])
        self._ep_writer = csv.DictWriter(self._ep_f,
            fieldnames=["timesteps","env_idx","ep_return","ep_len","done_reason"])
        self._step_writer.writeheader()
        self._ep_writer.writeheader()

    def _on_step(self) -> bool:
        # infos por cada env en este step
        dones = self.locals.get("dones", [])
        infos = self.locals.get("infos", [])
        for env_idx, (d, info) in enumerate(zip(dones, infos)):
            kpi = info.get("kpi")
            if kpi:
                row = {
                    "timesteps": int(self.num_timesteps),
                    "env_idx": env_idx,
                    "reward": kpi.get("reward", 0.0),
                    "roll": kpi.get("roll", 0.0),
                    "pitch": kpi.get("pitch", 0.0),
                    "left_down": kpi.get("left_down", 0),
                    "right_down": kpi.get("right_down", 0),
                    "F_L": kpi.get("F_L", 0.0),
                    "F_R": kpi.get("F_R", 0.0),
                    "zmp_x": kpi.get("zmp_x", 0.0),
                    "zmp_y": kpi.get("zmp_y", 0.0),
                    "com_x": kpi.get("com_x", 0.0),
                    "com_y": kpi.get("com_y", 0.0),
                    "com_z": kpi.get("com_z", 0.0),
                }
                self._step_writer.writerow(row)
            if d and "ep_kpi" in info:
                ep = info["ep_kpi"]
                self._ep_writer.writerow({
                    "timesteps": int(self.num_timesteps), "env_idx": env_idx,
                    "ep_return": ep.get("ep_return", 0.0),
                    "ep_len": ep.get("ep_len", 0),
                    "done_reason": ep.get("done_reason", None),
                })
        return True

    def _on_rollout_end(self) -> None:
        # algunos episodios terminan aquÃ­; SB3 no siempre pasa ep_info,
        # pero nosotros miramos en infos por si hay 'ep_kpi'
        ep_infos = self.locals.get("infos", [])
        for env_idx, info in enumerate(ep_infos):
            ep_kpi = info.get("ep_kpi")
            if ep_kpi:
                row = {
                    "timesteps": int(self.num_timesteps),
                    "env_idx": env_idx,
                    "ep_return": ep_kpi.get("ep_return", 0.0),
                    "ep_len": ep_kpi.get("ep_len", 0),
                    "done_reason": ep_kpi.get("done_reason", None),
                }
                self._ep_writer.writerow(row)

    def _on_training_end(self) -> None:
        try:
            if self._step_f: self._step_f.close()
            if self._ep_f: self._ep_f.close()
        except Exception:
            pass

"""
KPI CHECKLIST (entrenamiento PAM â€“ bÃ­pede)
- Ã‰xito L3 por pierna (%), duraciÃ³n single-support (media/p95)
- Tilt p95 / pico, ZMP margin p05 (>0 ideal), rate de toe-touch (<5%)
- No-support time mÃ¡x (<0.2 s), slip del pie de soporte
- SaturaciÃ³n PAM (<25%), co-contracciÃ³n, ||Î”u|| (suavidad)
- Cap-hit rate (L3=7.0) < 30%, acciones en borde < 30%
- PPO: entropy, approx_kl, clip_fraction, explained_variance
Alertas (3 ventanas seguidas):
- Ã©xito â†“ 5pp, tilt_p95 > 20Â°, KL > 2Ã—obj o clip_fraction > 0.6, FPS cae >30%
"""
class Simplified_Lift_Leg_Trainer:
    """
    Entrenador unificado mejorado para sistemas de robot bÃ­pedo con mÃºsculos PAM antagÃ³nicos.
    
    Esta versiÃ³n mejorada del entrenador entiende la diferencia fundamental entre:
    - Sistemas de mÃºsculos simples (4 PAMs independientes)
    - Sistemas de mÃºsculos antagÃ³nicos (12 PAMs coordinados biomecÃ¡nicamente)
    
    Es como la diferencia entre entrenar a alguien para mover mÃºsculos individuales
    versus entrenar a un atleta para coordinar grupos musculares de manera natural.
    """
    
    def __init__(self, 
                 total_timesteps=2000000,
                 n_envs=4,
                 learning_rate=3e-4,  # Ligeramente reducido para mayor estabilidad
                 resume_from=None,
                 logger=None,
                 csvlog=None,
                 _simple_reward_mode="progressive",  # Modo de recompensa por defecto
                 _allow_hops=False,
                 _vx_target=0.6
                 ):
        
        # ===== CONFIGURACIÃ“N BÃSICA =====
        
        self.env_type = "simplified_lift_legs"
        self.total_timesteps = total_timesteps
        self.n_envs = n_envs
        self.logger=logger
        self.learning_rate = learning_rate
        self.resume_from = resume_from
        self.csvlog=csvlog
        self.simple_reward_mode = _simple_reward_mode
        self.allow_hops = _allow_hops
        self.vx_target=_vx_target

        # Configurar el entorno y modelo segÃºn el tipo de sistema
        self._configuracion_modelo_entrenamiento()
        if self.logger:
            self.logger.log("main",f"ðŸ¤– simplified_lift_legs Trainer initialized")
            self.logger.log("main",f"   Target: Balance + Sentadillas con 6 PAMs")
            self.logger.log("main",f"   Timesteps: {self.total_timesteps:,}")
            self.logger.log("main",f"   Parallel envs: {self.n_envs}")
            self.logger.log("main",f"   Learning rate: {self.learning_rate}")
            self.logger.log("main",f"   Metodo de recompensa: {self.simple_reward_mode}")
            self.logger.log("main",f"   Permite saltos: {self.allow_hops}")
            self.logger.log("main",f"   Velocidad en caso de marcha: {self.vx_target}")

        # MANTENER EN CONSOLA SOLO CONFIRMACIÃ“N
        print(f"ðŸ¤– Trainer ready")
    

    def _configuracion_modelo_entrenamiento(self):
        """
            ConfiguraciÃ³n del modelo de entrenamiento adaptada para sistemas antagÃ³nicos.
            
            Esta funciÃ³n establece todas las configuraciones especÃ­ficas que necesita
            cada tipo de sistema. Es como configurar diferentes programas de entrenamiento
            para atletas de diferentes niveles.
        """

        # ===== CONFIGURACIÃ“N DE DIRECTORIOS =====
        
        # Crear directorios base
        self.model_dir = "./models_lift_leg"
        self.logs_dir = "./logs_lift_leg"
        self.checkpoints_dir = os.path.join(self.model_dir, "checkpoints")

        # Creacion directorios si no existen
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.logs_dir, exist_ok=True)
        os.makedirs(self.checkpoints_dir, exist_ok=True)
        
        # ===== CONFIGURACIONES ENTORNO =====
        
        # ConfiguraciÃ³n optimizada para sistemas de 6 mÃºsculos antagÃ³nicos
        self.env_config = {
                'clip_obs': 10.0,      
                'clip_reward': 15.0,   
                'model_prefix': 'single_leg_balance_pam',
                'description': 'lift_legs with 6 PAMs + Auto Knee Control'
        }
        # TambiÃ©n mantener el plural para compatibilidad interna
        self.env_configs = self.env_config

        # ===== ARQUITECTURA LSTM Sencilla=====
        
        # Sistemas antagÃ³nicos requieren mÃ¡s memoria temporal y capacidad de procesamiento
        self.policy_kwargs_lstm = {
            'lstm_hidden_size': 128,  # MÃ¡s Simplifico memoria
            'n_lstm_layers': 1,       # Capas simples
            'shared_lstm': False,     # LSTMs separados para policy y value
        }

        # ===== INFORMACIÃ“N DE ENTRENAMIENTO =====
        
        self.training_info = {
            'completed_timesteps': 0,
            'last_checkpoint': None,
            'training_start_time': None,
            'total_training_time': 0
        }

    def create_training_env(self):
        """
        CreaciÃ³n de entorno de entrenamiento para sentadillas.
        
        """
        
        config = self.env_configs
        n_envs_local = int(self.n_envs)
        logger_local = (self.logger if (self.logger and n_envs_local == 1) else None)
        # Se puede pasar puramente a csvlog
        csvlog_local= (self.csvlog if (self.csvlog and n_envs_local == 1) else None)
        _simple_reward_mode_local=self.simple_reward_mode
        _allow_hops_local=self.allow_hops
        _vx_target_local=self.vx_target
        if logger_local and n_envs_local==1:
            self.logger.log("main",f"ðŸ—ï¸ Creating training environment: {config['description']}")
        def make_env(logger=logger_local, csvlog=csvlog_local,
                     n_envs=n_envs_local,_simple_reward_mode=_simple_reward_mode_local,
                     _allow_hops=_allow_hops_local, _vx_target=_vx_target_local):
            def _init():
                # Crear el entorno con la configuraciÃ³n apropiada
                env = Simple_Lift_Leg_BipedEnv(
                    logger=logger,
                    csvlog=csvlog,
                    render_mode='human' if n_envs == 1 else 'direct', 
                    print_env="TRAIN",  # Para diferenciar en logs
                    simple_reward_mode=_simple_reward_mode,
                    allow_hops=_allow_hops,
                    vx_target=_vx_target
                    
                )
                #Eliminado escribir , os.path.join(self.logs_dir, f"train_worker_{rank}" acelero entrenamiento
                env = Monitor(env)
                return env
            return _init
        
        # Crear entornos paralelos con configuraciÃ³n optimizada
        env = self._create_parallel_env(make_env=make_env, config=config)
        
        return env

    
    def create_eval_env(self):
        """
            Crear entorno de evaluaciÃ³n.
       
        """
        eval_csvlog = CSVLogger(
            timestamp=getattr(self.csvlog, "timestamp", None),  # reutiliza el TS si ya existe
            only_workers=False                                  # â† importante: que escriba en main
        )
        def make_eval_env():
            def _init():
                env = Simple_Lift_Leg_BipedEnv(render_mode='direct', 
                                            print_env="EVAL",
                                            logger=self.logger,
                                            csvlog=eval_csvlog,
                                            simple_reward_mode=self.simple_reward_mode,
                                            allow_hops=self.allow_hops,
                                            vx_target=self.vx_target

                                            )  # Fase de evaluaciÃ³n es balance
                env = Monitor(env, os.path.join(self.logs_dir, "eval"))
                return env
            return _init
        
        eval_env = DummyVecEnv([make_eval_env()])
        eval_env = VecNormalize(eval_env, 
                               norm_obs=True, 
                               norm_reward=True, 
                               clip_obs=self.env_configs['clip_obs'], 
                               training=False)
        # NEW: cargar stats de normalizaciÃ³n del train si existen
        norm_path = os.path.join(self.model_dir, f"{self.env_configs['model_prefix']}_normalize.pkl")
        if os.path.exists(norm_path):
            try:
                eval_env = VecNormalize.load(norm_path, eval_env)
                eval_env.training = False          # importante para que no actualice stats
                eval_env.norm_reward = False       # opcional: no normalizar reward en eval
            except Exception as e:
                print(f"âš ï¸ Could not load eval normalization: {e}")

        return eval_env
    
    def create_model(self, env, resume_path=None):
        """
            Crear modelo RecurrentPPO optimizado para sistemas antagÃ³nicos.
            
            Los sistemas de mÃºsculos antagÃ³nicos requieren arquitecturas de red
            mÃ¡s sofisticadas y hiperparÃ¡metros ajustados para manejar la complejidad
            adicional de la coordinaciÃ³n muscular.
        """
        
        # Intentar cargar modelo existente. Se escribe asÃ­ para ahorrar espacio y legibilidad.
        model = self.cargar_creacion_modelo(resume_path=resume_path, env=env)
        if model is not None:
            return model

        print(f"ðŸ§  Creating new RecurrentPPO model for lift_leg...")
        # ===== CREACIÃ“N DEL MODELO =====
        model_params = {
            'learning_rate': self.learning_rate,
            'gamma': 0.99,             # EstÃ¡ndar
            'max_grad_norm': 0.5,      # EstÃ¡ndar
            'ent_coef': 0.01,          # ExploraciÃ³n moderada
            'n_steps': 256,            # Reducido
            'batch_size': 128,         # Reducido
            'n_epochs': 4,             # Reducido
            'gae_lambda': 0.95,        # EstÃ¡ndar
            'clip_range': 0.2,         # EstÃ¡ndar
            'vf_coef': 0.5,            # EstÃ¡ndar
        }
        
        model = RecurrentPPO(
            "MlpLstmPolicy",
            env,
            policy_kwargs=self.policy_kwargs_lstm,
            verbose=1,
            tensorboard_log=self.logs_dir,
            device='auto',
            **model_params
        )
        
        print(f"âœ… Model created with {self.policy_kwargs_lstm['lstm_hidden_size']} LSTM units")
        
        return model
    
    def setup_callbacks(self, eval_env):
        """
            Configurar callbacks optimizados para sistemas antagÃ³nicos.
            
            Los sistemas antagÃ³nicos requieren monitoreo mÃ¡s frecuente y mÃ©tricas
            mÃ¡s sofisticadas para entender el progreso del entrenamiento.
        """
        
        config = self.env_configs
        
        # ===== CHECKPOINT CALLBACK =====
        
        # Sistemas antagÃ³nicos se benefician de checkpoints mÃ¡s frecuentes
        checkpoint_freq = 90000//self.n_envs  # Cada 100k timesteps dividido por el nÃºmero de entornos
        
        checkpoint_callback = CheckpointCallback(
            save_freq=checkpoint_freq,
            save_path=self.checkpoints_dir,
            name_prefix=f'{config["model_prefix"]}_checkpoint',
            verbose=1
        )
        
        # ===== EVALUATION CALLBACK =====
        
        # EvaluaciÃ³n mÃ¡s frecuente para sistemas complejos
        eval_freq = 30000 //self.n_envs
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=self.model_dir,
            log_path=os.path.join(self.logs_dir, "eval"),
            eval_freq=eval_freq,
            deterministic=True,
            render=False,
            verbose=1
        )
        kpi_csv_cb = SimpleCsvKpiCallback(logs_dir=self.logs_dir, verbose=0)
        callbacks = CallbackList([checkpoint_callback, eval_callback, kpi_csv_cb])
        
        
        return callbacks
        
    
    def train(self, resume=True):
        """
            Entrenamiento DIRECTO y SIMPLIFICADO para balance/sentadillas.
            
            Sin fases complejas, sin curriculum, solo entrenamiento RecurrentPPO directo.
        """
        
        print(f"ðŸš€ Starting lift_leg training with RecurrentPPO...")

        # DETALLES AL LOG
        if self.logger:
            self.logger.log("main","ðŸš€ Training session started")
            self.logger.log("main",f"   Resume: {resume}")
        
        # ===== PREPARACIÃ“N DEL ENTRENAMIENTO =====
        
        resume_path, resume_timesteps, remaining_timesteps = self.prep_modelo_anterior(resume)
        if resume_path is None and remaining_timesteps is None:
            print("   âœ… Training already completed!")
            return

        # ===== CREACIÃ“N DE ENTORNOS =====
        
        train_env = self.create_training_env()
        eval_env = self.create_eval_env()
        
        # Cargar normalizaciones existentes si las hay
        train_env = cargar_posible_normalizacion(self.model_dir, resume_path, self.env_configs, train_env)
        
        # ===== CREACIÃ“N DEL MODELO =====
        
        model = self.create_model(train_env, resume_path)
        
        # ===== CONFIGURACIÃ“N DE CALLBACKS =====
        
        callbacks = self.setup_callbacks(eval_env)
        
        # ===== REGISTRO DE INICIO =====
        
        self.training_info['training_start_time'] = datetime.now().isoformat()
        self.training_info['completed_timesteps'] = resume_timesteps
        
        try:
            # ===== EJECUTAR ENTRENAMIENTO =====
            
            # Entrenamiento tradicional sin currÃ­culo
            model.learn(
                total_timesteps=remaining_timesteps,
                callback=callbacks,
                tb_log_name=f"{self.env_configs['model_prefix']}_training",
                reset_num_timesteps=(resume_timesteps == 0)
            )
            self.training_info['completed_timesteps'] = self.total_timesteps
            
            # ===== GUARDAR MODELO FINAL =====
            
            final_model_path = os.path.join(self.model_dir, f"{self.env_configs['model_prefix']}_final")
            model.save(final_model_path)
            print(f"âœ… Final model saved at: {final_model_path}")
            
            # Guardar normalizaciones
            if isinstance(train_env, VecNormalize):
                norm_path = os.path.join(self.model_dir, f"{self.env_configs['model_prefix']}_normalize.pkl")
                train_env.save(norm_path)
                print(f"âœ… Normalization saved at: {norm_path}")
            
            # ===== GUARDAR INFORMACIÃ“N DE ENTRENAMIENTO =====

            self._save_training_info()
            
            print(f"\nðŸŽ‰ lift_leg training completed successfully!")
            print(f"   Total timesteps: {self.total_timesteps:,}")
            print(f"   Model saved in: {self.model_dir}")
            # Ã‰XITO: A AMBOS
            both_print(f"âœ… Training completed successfully!")
            
            return model
                
        except KeyboardInterrupt:
            both_print("â¸ï¸ Training interrupted by user")

        except Exception as e:
            both_print(f"âŒ Training failed: {e}")
            
        finally:
            train_env.close()
            eval_env.close()
    

    def cargar_creacion_modelo(self, resume_path, env):
        """Cargando el modelo, movio aqui por legibilidad"""
        if resume_path and os.path.exists(resume_path):
            print(f"ðŸ”„ Loading model from checkpoint: {resume_path}")
            try:
                model = RecurrentPPO.load(resume_path, env=env)
                
                if hasattr(model, 'learning_rate') and model.learning_rate != self.learning_rate:
                    print(f"ðŸ“ Updating learning rate: {model.learning_rate} -> {self.learning_rate}")
                    model.learning_rate = self.learning_rate
                    
                return model
            except Exception as e:
                print(f"âš ï¸ Error loading model: {e}")
                print("ðŸ”„ Creating new model instead...")

                
        return None
    
    def _save_training_info(self):
        """Guardar informaciÃ³n bÃ¡sica del entrenamiento"""
        
        info_path = os.path.join(self.model_dir, f"{self.env_configs['model_prefix']}_info.json")
        
        training_data = {
            'objective': 'lift_leg with 6 PAM muscles',
            'environment': self.env_type,
            'completed_timesteps': self.training_info['completed_timesteps'],
            'total_timesteps_target': self.total_timesteps,
            'training_start': self.training_info['training_start_time'],
            'n_envs': self.n_envs,
            'learning_rate': self.learning_rate,
            'lstm_config': self.policy_kwargs_lstm,
            'simplified_trainer': True
        }
        
        with open(info_path, 'w') as f:
            json.dump(training_data, f, indent=2)
        
        print(f"ðŸ’¾ Training info saved: {info_path}")

    def prep_modelo_anterior(self, resume):
        """Carga el modelo anterior teniendo en cuenta los timesteps restantes"""
        resume_path = None
        resume_timesteps = 0

        if resume and self.resume_from:
            if os.path.exists(self.resume_from):
                resume_path = self.resume_from
                match = re.search(r'_(\d+)_steps\.zip$', self.resume_from)
                if match:
                    resume_timesteps = int(match.group(1))
                print(f"ðŸ“‚ Resuming from specified checkpoint: {self.resume_from}")
            else:
                print(f"âš ï¸ Specified checkpoint not found: {self.resume_from}")
        elif resume:
            latest_checkpoint, resume_timesteps = self._find_latest_checkpoint()
            if latest_checkpoint:
                resume_path = latest_checkpoint
        
        remaining_timesteps = max(0, self.total_timesteps - resume_timesteps)

        if remaining_timesteps == 0:
            print(f"âœ… Training already completed! ({resume_timesteps:,}/{self.total_timesteps:,} timesteps)")
            return None, None, None
        
        print(f"ðŸ“Š Training plan:")
        print(f"   Completed: {resume_timesteps:,}")
        print(f"   Remaining: {remaining_timesteps:,}")
        print(f"   Total target: {self.total_timesteps:,}")
        
        return resume_path, resume_timesteps, remaining_timesteps

    def _find_latest_checkpoint(self):
        """Find the most recent checkpoint based on timesteps."""
        model_prefix = self.env_configs['model_prefix']
        
        checkpoint_pattern = os.path.join(self.checkpoints_dir, f"{model_prefix}_checkpoint_*_steps.zip")
        checkpoint_files = glob.glob(checkpoint_pattern)
        
        if not checkpoint_files:
            return None, 0
        
        timesteps = []
        for file in checkpoint_files:
            match = re.search(r'_(\d+)_steps\.zip$', file)
            if match:
                timesteps.append((int(match.group(1)), file))
        
        if not timesteps:
            return None, 0
        
        latest_timesteps, latest_file = max(timesteps, key=lambda x: x[0])
        return latest_file, latest_timesteps
    
# ========================================================================================================= #
# =====================================creacion de entornos paralelos====================================== #
# ========================================================================================================= #

    def _create_parallel_env(self, make_env, config):
        """CreaciÃ³n de entornos paralelos optimizada"""
        if self.n_envs > 1:
            base_env = SubprocVecEnv([make_env() for i in range(self.n_envs)])
            env = VecNormalize(base_env, 
                             norm_obs=True, 
                             norm_reward=True, 
                             clip_obs=config['clip_obs'],
                             clip_reward=config['clip_reward'])
        else:
            base_env = DummyVecEnv([make_env()])
            env = VecNormalize(base_env, 
                             norm_obs=True, 
                             norm_reward=True, 
                             clip_obs=config['clip_obs'],
                             clip_reward=config['clip_reward'])

        return env
    
    # Ya que he creado clase sin RL para testing, tambiÃ©n creo con este un modelo de entrenamiento sin RL

def create_balance_leg_trainer_no_curriculum(total_timesteps=1000000, n_envs=4, learning_rate=3e-4,logger=None, csvlog=None):
    """
    FunciÃ³n para crear fÃ¡cilmente un entrenador SIN curriculum
    """
    
    trainer = Simplified_Lift_Leg_Trainer(
        total_timesteps=total_timesteps,
        n_envs=n_envs,
        learning_rate=learning_rate,
        logger=logger,
        csvlog=csvlog
    )
    
    print(f"âœ… Trainer created (NO CURRICULUM)")
    print(f"   Focus: Balance bÃ¡sico con RL puro")
    print(f"   Expert help: 0% (assist=0 siempre)")
    print(f"   Architecture: RecurrentPPO with {trainer.policy_kwargs_lstm['lstm_hidden_size']} LSTM units")
    
    return trainer

# ===== Helpers para lanzar entrenos por modo =====
#Ejemplos de uso, (No parece buena idea usarlos asÃ­, mejor creo versiones mias
def create_march_in_place_trainer(total_timesteps=1_500_000, n_envs=4, learning_rate=3e-4, logger=None, csvlog=None):
    trainer = Simplified_Lift_Leg_Trainer(total_timesteps=total_timesteps, n_envs=n_envs, learning_rate=learning_rate, logger=logger, csvlog=csvlog)
    original = trainer.create_training_env
    def patched():
        cfg = trainer.env_configs
        def make_env(logger=None, csvlog=None, n_envs=n_envs):
            return (lambda: Simple_Lift_Leg_BipedEnv(logger=logger, csvlog=csvlog,
                                                     render_mode='human' if n_envs==1 else 'direct',
                                                     print_env='TRAIN',
                                                     simple_reward_mode='march_in_place',
                                                     allow_hops=True,
                                                     vx_target=0.0))
        # adaptamos a la firma interna del trainer (_create_parallel_env espera make_env(rank)=>callable)
        return trainer._create_parallel_env(make_env=lambda rank=0: make_env(logger, csvlog), config=cfg)
    trainer.create_training_env = patched
    return trainer


def create_walk3d_trainer(total_timesteps=2_000_000, n_envs=4, learning_rate=3e-4, vx_target=0.6, logger=None, csvlog=None):
    trainer = Simplified_Lift_Leg_Trainer(total_timesteps=total_timesteps, n_envs=n_envs, learning_rate=learning_rate, logger=logger, csvlog=csvlog)
    original = trainer.create_training_env
    def patched():
        cfg = trainer.env_configs
        def make_env(logger=None, csvlog=None, n_envs=n_envs, vx=vx_target):
            return (lambda: Simple_Lift_Leg_BipedEnv(logger=logger, csvlog=csvlog,
                                                     render_mode='human' if n_envs==1 else 'direct',
                                                     print_env='TRAIN',
                                                     simple_reward_mode='walk3d',
                                                     allow_hops=False,
                                                     vx_target=vx))
        return trainer._create_parallel_env(make_env=lambda rank=0: make_env(logger, csvlog), config=cfg)
    trainer.create_training_env = patched
    return trainer